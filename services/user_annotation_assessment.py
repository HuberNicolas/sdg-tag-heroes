import json
import time
from typing import Literal, Dict
from pydantic import BaseModel, Field
from openai import OpenAI
import instructor
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

from settings.settings import UserAnnotationAssessmentSettings
from utils.env_loader import load_env, get_env_variable
from settings.sdg_descriptions import sdgs

import os
# OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.
os.environ["OPENBLAS_NUM_THREADS"] = "1"

# Load the API environment variables
load_env('api.env')

user_annotation_assessment_settings = UserAnnotationAssessmentSettings()
client = instructor.from_openai(OpenAI(api_key=get_env_variable('OPENAI_API_KEY')))
MODEL = user_annotation_assessment_settings.GPT_MODEL

# Load Transformer Model
tokenizer = AutoTokenizer.from_pretrained(user_annotation_assessment_settings.BERT_PRETRAINED_MODEL_NAME)
model = AutoModel.from_pretrained(user_annotation_assessment_settings.BERT_PRETRAINED_MODEL_NAME)

class AnnotationScoreResponse(BaseModel):
    relevance: float = Field(description="Relevance score (0-5).")
    depth: float = Field(description="Depth score (0-5).")
    correctness: float = Field(description="Correctness score (0-5).")
    creativity: float = Field(description="Creativity score (0-5).")
    reasoning: str = Field(description="Short and concise description of the reasoning of the assessment.")
    llm_score: float = Field(description="Overall average score generated by the LLM-based approach.")
    semantic_score: float = Field(description="Score generated by the semantic-based approach.")
    combined_score: float = Field(description="Weighted average of the LLM and semantic scores.")

class UserAnnotationAssessment():
    """
    Generates enriched user descriptions and evaluates knowledge internalization using LLM and semantic-based methods.
    """
    def __init__(self):
        self.context = (
             "You are a helpful assistant that scores user annotations based on their relevance, depth, correctness, and creativity."
        )

    def _call_model(self, prompt_data: dict, response_model):
        """
        Handles the API call with the Instructor client.

        Args:
            prompt_data (dict): The prompt data for the model.
            response_model (BaseModel): The Pydantic model to validate the response.

        Returns:
            BaseModel: The validated response parsed into the specified Pydantic model.
        """
        response = client.beta.chat.completions.parse(
            model=MODEL,
            messages=[
                {"role": "system", "content": self.context},
                {"role": "user", "content": json.dumps(prompt_data)}
            ],
            response_format=response_model,
        )
        return response.choices[0].message.parsed

    def calculate_llm_score(self, passage: str, annotation: str, sdg_label: str) -> Dict[str, int | str]:
        """
        Calculates the LLM-based score for an annotation and provides a short and concise reasoning.

        Args:
            passage (str): The marked passage.
            annotation (str): The user's comment.
            sdg_label (str): The SDG label for evaluation.

        Returns:
            int: The LLM-based score.
        """
        prompt_data = {
            "instruction": (
                "Evaluate the following annotation for its relevance, depth, correctness, and creativity based on the marked passage and SDG label."
                "Provide a score from 0-10 (integer) for each of the following dimensions:"
                "1. Relevance: How well does the annotation align with the SDG label and passage?"
                "2. Depth: Does the annotation provide insightful, detailed reasoning or evidence?"
                "3. Correctness: Is the information accurate and logically sound?"
                "4. Creativity: Does the annotation showcase originality or unique perspectives?"
                "Finally, give a short and precise reasoning using minimal words for maximal information for your assessment."
            ),
            "passage": passage,
            "annotation": annotation,
            "sdg_label": sdg_label
        }
        response = self._call_model(prompt_data, AnnotationScoreResponse)
        return {
            "relevance": response.relevance,
            "depth": response.depth,
            "correctness": response.correctness,
            "creativity": response.creativity,
            "reasoning": response.reasoning,
        }

    def calculate_semantic_similarity(self, annotation: str, sdg_label: int) -> float:
        """
        Calculates semantic similarity between the annotation and the corresponding SDG description.

        Args:
            annotation (str): The user's comment.
            sdg_label (int): The index of the SDG label (1 to 17).

        Returns:
            float: The semantic similarity score.
        """
        sdg_description = next((sdg.sdg_description for sdg in sdgs if sdg.index == sdg_label), None)
        if not sdg_description:
            raise ValueError(f"Invalid SDG index: {sdg_label}")

        # Encode annotation and SDG description
        start = time.time()
        annotation_embedding = model(**tokenizer(annotation, return_tensors="pt", truncation=True, padding=True))
        end = time.time()
        print(f"Annotation embedding time: {end - start}")

        start = time.time()
        sdg_embedding = model(**tokenizer(sdg_description, return_tensors="pt", truncation=True, padding=True))
        end = time.time()
        print(f"SDG embedding time: {end - start}")

        # Extract CLS token embeddings
        start = time.time()
        annotation_vector = annotation_embedding.last_hidden_state[:, 0, :].squeeze()
        end = time.time()
        print(f"Annotation vector time: {end - start}")

        start = time.time()
        sdg_vector = sdg_embedding.last_hidden_state[:, 0, :].squeeze()
        end = time.time()
        print(f"SDG vector time: {end - start}")

        # Compute cosine similarity
        start = time.time()
        similarity = torch.nn.functional.cosine_similarity(annotation_vector, sdg_vector, dim=0).item()
        end = time.time()
        print(f"Cosine similarity time: {end - start}")
        return similarity

    def evaluate_annotation(self, passage: str, annotation: str, sdg_label: int, llm_weight: float = 0.5, semantic_weight: float = 0.5) -> AnnotationScoreResponse:
        """
        Combines LLM and transformer-based scores for final evaluation.

        Args:
            passage (str): The marked passage.
            annotation (str): The user's comment.
            sdg_label (int): The SDG label index for evaluation.
            llm_weight (float): Weight for the LLM-based score.
            semantic_weight (float): Weight for the semantic similarity score.

        Returns:
            AnnotationScoreResponse: The combined score and individual scores.
        """
        llm_scores = self.calculate_llm_score(passage, annotation, sdg_label)
        semantic_score = self.calculate_semantic_similarity(annotation, sdg_label)

        llm_total_score = (llm_scores["relevance"] + llm_scores["depth"] + llm_scores["correctness"] + llm_scores[
            "creativity"]) / 4

        combined_score = (llm_weight * llm_total_score + semantic_weight * semantic_score) / (llm_weight + semantic_weight)

        return AnnotationScoreResponse(
            relevance=llm_scores["relevance"],
            depth=llm_scores["depth"],
            correctness=llm_scores["correctness"],
            creativity=llm_scores["creativity"],
            reasoning=llm_scores["reasoning"],
            llm_score=llm_total_score,
            semantic_score=semantic_score,
            combined_score=combined_score
        )
